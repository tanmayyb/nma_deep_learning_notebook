# Biological vs. Artificial Neural Networks

## Section1: Universal Approximation Theorem
<p>
How 9 Neurons can Approximate a sin function. We learned about Descision Boundaries/Non-Linearity and their impact.We also touched the XOR problem.Overall, this was meant to develop an intuition for the neural networks by encouragin us to explore it from the perspective of biases and weights.
</p>

**Potential Project:**
<p>Visualising the toy Neural Network (1 Input Neuron, 9 ReLu Functions, 1 Output Neuron)
</p>

## Section 2:  MLPs in Pytorch
<details>
<summary>MLP Creation Class</summary>
<br>

```
class Net(nn.Module):
  """
  Initialize MLP Network
  """

  def __init__(self, actv, input_feature_num, hidden_unit_nums, output_feature_num):
    """
    Initialize MLP Network parameters
    Args:
      actv: string
        Activation function
      input_feature_num: int
        Number of input features
      hidden_unit_nums: int
        Number of units in the hidden layer
      output_feature_num: int
        Number of output features
    Returns:
      Nothing
    """
    super(Net, self).__init__()
    self.input_feature_num = input_feature_num # Save the input size for reshaping later
    self.mlp = nn.Sequential() # Initialize layers of MLP

    in_num = input_feature_num # Initialize the temporary input feature to each layer
    for i in range(len(hidden_unit_nums)): # Loop over layers and create each one

      out_num = hidden_unit_nums[i] # Assign the current layer hidden unit from list
      layer = nn.Linear(in_num, out_num) # Use nn.Linear to define the layer
      in_num = out_num # Assign next layer input using current layer output
      self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model with a name

      actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)
      self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name

    out_layer = nn.Linear(in_num, output_feature_num) # Create final layer
    self.mlp.add_module('Output_Linear', out_layer) # Append the final layer

  def forward(self, x):
    """
    Simulate forward pass of MLP Network
    Args:
      x: torch.tensor
        Input data
    Returns:
      logits: Instance of MLP
        Forward pass of MLP
    """
    # Reshape inputs to (batch_size, input_feature_num)
    # Just in case the input vector is not 2D, like an image!
    x = x.view(-1, self.input_feature_num)

    logits = self.mlp(x) # Forward pass of MLP
    return logits

# Add event to airtable
atform.add_event('Coding Exercise 2: Implement a general-purpose MLP in Pytorch')


input = torch.zeros((100, 2))
## Uncomment below to create network and test it on input
net = Net(actv='LeakyReLU(0.1)', input_feature_num=2, hidden_unit_nums=[100, 10, 5], output_feature_num=1).to(DEVICE)
y = net(input.to(DEVICE))
print(f'The output shape is {y.shape} for an input of shape {input.shape}')
```
</details>


## Section 2.1:
### Summary
- Softmax takes Logits of last layer and converts them to Positive Numbers that sum to 1.
- Cross Entropy Loss =  Entropy of Y(ground truth) + KL-Divergence
- KL Divergence Minimises distance to the One Hot Encoded vectors in high dimensional space ([Probability Simplex](https://www.localmaxradio.com/questions/what-is-a-probability-simplex#:~:text=A%20probability%20simplex%20is%20a,denote%20the%20number%20of%20categories.)) (insert figure)

## Section 2.3: 

### Summary

<ol>
<li>How Training/Validating/Testing Datasets affects performance</br>
<li>How Evaluation Metrics affect performance</br>
<li>How Datasets can affect performance of an entire field of Research </br>
insert photo
<li>Bias at every step of pipeline</br>

</ol>

## Bonus: From Neuron to ReLu
LIF Derived from Hodgkinn-Huxley Model
insert photo here

[Harvard: Action Potential in the Neuron](https://www.youtube.com/watch?v=oa6rvUJlg7o&t=654s&ab_channel=HarvardExtensionSchool)

### Summary

<ul>
<li>Firing Rate of neuron is directly proportionnal to the intengrity of the input</br>
<li>Firing Rate vs Input Action Potential Plot resembles ReLu Plot</br>
</ul>