# W1D2T2 Learning Hyperparamaters


## section 1: Deep Linear Neural Nets

> single nonlinear hidden layer (given enough number of neurons and infinite training samples) has the potential to approximate any function
<p>
shallow nonlinear neural networks hardly meet their true potential in practice. In the contrast, deep neural nets are often surprisingly powerful in learning complex functions without sacrificing generalization. A core intuition behind deep learning is that deep nets derive their power through learning internal representations.</p>

![ref: A mathematical theory of semantic development in deep neural networks](https://www.pnas.org/doi/10.1073/pnas.1820226116)



> "We use a toy model that represents an "Exactly Heirarchical Model"

insertt hierarchy image

**Learning Semantic Properties**
![ref: Semantic cognition: A parallel distributed processing approach.](https://psycnet.apa.org/record/2004-18753-000)

![ref: Learning and connectionist representations.](https://psycnet.apa.org/record/1993-97600-001)


## section 2: singular value decomposition

> inits affect learning time and also the representations the mdodel  learns

insert learning diff image here

<p>
SVD 'untangles'/decouples the dense layers into parallel independent deep chains of neurons.to understanding the nonlinear learning dynamics of a deep LNN

</p>

insert image here

![ref: Gilbert Strang teaching SVD](https://www.youtube.com/watch?v=mBcLRGuAFUk&ab_channel=MITOpenCourseWare)

<p> 
Think! (what does this mean)

In EigenValue decomposition, the amount of variance explained by eigenvectors is proportional to the corresponding eigenvalues. What about the SVD? We see that the gradient descent guides the network to first learn the features that carry more information (have higher singular value)!
</p>